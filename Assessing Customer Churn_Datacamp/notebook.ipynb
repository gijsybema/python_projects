{"cells":[{"source":"# Assessing Customer Churn Using Machine Learning","metadata":{},"cell_type":"markdown","id":"522861ae-b7e2-4414-a5ef-514fbcdcb0b4"},{"source":"The telecommunications (telecom) sector in India is rapidly changing, with more and more telecom businesses being created and many customers deciding to switch between providers. \"Churn\" refers to the process where customers or subscribers stop using a company's services or products. Understanding the factors that influence keeping a customer as a client in predicting churn is crucial for telecom companies to enhance their service quality and customer satisfaction. As the data scientist on this project, you aim to explore the intricate dynamics of customer behavior and demographics in the Indian telecom sector in predicting customer churn, utilizing two comprehensive datasets from four major telecom partners: Airtel, Reliance Jio, Vodafone, and BSNL:\n\n- `telecom_demographics.csv` contains information related to Indian customer demographics:\n\n| Variable             | Description                                      |\n|----------------------|--------------------------------------------------|\n| `customer_id `         | Unique identifier for each customer.             |\n| `telecom_partner `     | The telecom partner associated with the customer.|\n| `gender `              | The gender of the customer.                      |\n| `age `                 | The age of the customer.                         |\n| `state`                | The Indian state in which the customer is located.|\n| `city`                 | The city in which the customer is located.       |\n| `pincode`              | The pincode of the customer's location.          |\n| `registration_event` | When the customer registered with the telecom partner.|\n| `num_dependents`      | The number of dependents (e.g., children) the customer has.|\n| `estimated_salary`     | The customer's estimated salary.                 |\n\n- `telecom_usage` contains information about the usage patterns of Indian customers:\n\n| Variable   | Description                                                  |\n|------------|--------------------------------------------------------------|\n| `customer_id` | Unique identifier for each customer.                         |\n| `calls_made` | The number of calls made by the customer.                    |\n| `sms_sent`   | The number of SMS messages sent by the customer.             |\n| `data_used`  | The amount of data used by the customer.                     |\n| `churn`    | Binary variable indicating whether the customer has churned or not (1 = churned, 0 = not churned).|\n","metadata":{},"id":"dafa483a-e084-4ba8-9236-5c0468364e0d","cell_type":"markdown"},{"source":"### Project instructions\nDoes Logistic Regression or Random Forest produce a higher accuracy score in predicting telecom churn in India?","metadata":{},"cell_type":"markdown","id":"d6d72108-8963-47df-9517-ddf2bd29e9bc"},{"source":"# Import libraries and methods/functions\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1755869442016,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import libraries and methods/functions\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score","lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b"},"id":"95efd3c7-a48a-49c2-9df6-36f078de3b38","cell_type":"code","execution_count":101,"outputs":[]},{"source":"### 1. Loading and exploring data","metadata":{},"cell_type":"markdown","id":"f064ec4f-4b22-4a59-a5dc-8537b34549b1"},{"source":"# load datasets\ndf_demographics = pd.read_csv('telecom_demographics.csv')\n#df_demographics.info()\n#print(df_demographics.head())\n\ndf_usage = pd.read_csv('telecom_usage.csv')\n#df_usage.info()\n#print(df_usage.head())\n\n# merge datasets\nchurn_df = pd.merge(df_demographics, df_usage, on='customer_id')\n#churn_df.info()\n\n# calculate proportion of customers who have churned\nprint(f\"Number of customers churned: {str(churn_df['churn'].sum())}\")\nprint(f\"Proportion of customers churned: {str(churn_df['churn'].mean())}\")","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1755869442068,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# load datasets\ndf_demographics = pd.read_csv('telecom_demographics.csv')\n#df_demographics.info()\n#print(df_demographics.head())\n\ndf_usage = pd.read_csv('telecom_usage.csv')\n#df_usage.info()\n#print(df_usage.head())\n\n# merge datasets\nchurn_df = pd.merge(df_demographics, df_usage, on='customer_id')\n#churn_df.info()\n\n# calculate proportion of customers who have churned\nprint(f\"Number of customers churned: {str(churn_df['churn'].sum())}\")\nprint(f\"Proportion of customers churned: {str(churn_df['churn'].mean())}\")","outputsMetadata":{"0":{"height":59,"type":"stream"},"1":{"height":500,"type":"dataFrame","tableState":{}}}},"cell_type":"code","id":"fc86c0aa-b366-4e6d-b08e-994e9bc71787","outputs":[{"output_type":"stream","name":"stdout","text":"Number of customers churned: 1303\nProportion of customers churned: 0.20046153846153847\n"}],"execution_count":102},{"source":"#churn_df.describe()","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1755869442114,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#churn_df.describe()","outputsMetadata":{"0":{"height":500,"type":"dataFrame","tableState":{}}}},"cell_type":"code","id":"6b69667b-249a-45ba-9718-2fbe36bcdf5c","outputs":[],"execution_count":103},{"source":"### 2. Processing data\n- define target variable\n- convert categorical data to numerical representation\n- handle datetime columns\n- remove redundant features\n- standardize relevant features","metadata":{},"cell_type":"markdown","id":"9967acfd-c513-434f-ac32-571ed336e66d"},{"source":"# Define target variable\ntarget = churn_df['churn']","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1755869442163,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define target variable\ntarget = churn_df['churn']"},"cell_type":"code","id":"057ed9cb-5743-4551-a8a8-81c582c70f11","outputs":[],"execution_count":104},{"source":"# Determine categorical variables\ncategorical_features = ['telecom_partner', 'gender', 'state', 'city']\n#for feature in categorical_features:\n#    display(churn_df[feature].value_counts())\n\n# convert categorical variables to numeric\ndummies = pd.get_dummies(churn_df[categorical_features], drop_first=True)\n#dummies.shape\n#dummies.head()\n\n# change registration_event to datetime\nchurn_df['registration_event'] = pd.to_datetime(churn_df['registration_event'])\n\n# calculate days_since_registration \nchurn_df['days_since_registration'] = (datetime.now() - churn_df['registration_event']).dt.days\n\n# Drop irrelevant columns\n\n# drop registration_event column\nchurn_df = churn_df.drop('registration_event', axis='columns')\n\n# drop column 'pincode'\n#churn_df['pincode'].value_counts()\nchurn_df = churn_df.drop('pincode', axis='columns')\n\n# drop columns 'churn' and 'customer_id'\nchurn_df = churn_df.drop(['churn', 'customer_id'], axis='columns')\nprint(churn_df.shape)\nchurn_df.head()","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1755869442216,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Determine categorical variables\ncategorical_features = ['telecom_partner', 'gender', 'state', 'city']\n#for feature in categorical_features:\n#    display(churn_df[feature].value_counts())\n\n# convert categorical variables to numeric\ndummies = pd.get_dummies(churn_df[categorical_features], drop_first=True)\n#dummies.shape\n#dummies.head()\n\n# change registration_event to datetime\nchurn_df['registration_event'] = pd.to_datetime(churn_df['registration_event'])\n\n# calculate days_since_registration \nchurn_df['days_since_registration'] = (datetime.now() - churn_df['registration_event']).dt.days\n\n# Drop irrelevant columns\n\n# drop registration_event column\nchurn_df = churn_df.drop('registration_event', axis='columns')\n\n# drop column 'pincode'\n#churn_df['pincode'].value_counts()\nchurn_df = churn_df.drop('pincode', axis='columns')\n\n# drop columns 'churn' and 'customer_id'\nchurn_df = churn_df.drop(['churn', 'customer_id'], axis='columns')\nprint(churn_df.shape)\nchurn_df.head()","outputsMetadata":{"0":{"height":500,"type":"dataFrame","tableState":{}},"1":{"height":500,"type":"dataFrame","tableState":{}},"2":{"height":500,"type":"dataFrame","tableState":{}},"3":{"height":500,"type":"dataFrame","tableState":{}},"4":{"height":500,"type":"dataFrame","tableState":{}},"5":{"height":500,"type":"dataFrame","tableState":{}}}},"cell_type":"code","id":"c3925f37-7123-47e4-804b-b6c5017fbd3a","outputs":[{"output_type":"stream","name":"stdout","text":"(6500, 11)\n"},{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"telecom_partner","type":"string"},{"name":"gender","type":"string"},{"name":"age","type":"integer"},{"name":"state","type":"string"},{"name":"city","type":"string"},{"name":"num_dependents","type":"integer"},{"name":"estimated_salary","type":"integer"},{"name":"calls_made","type":"integer"},{"name":"sms_sent","type":"integer"},{"name":"data_used","type":"integer"},{"name":"days_since_registration","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2,3,4],"telecom_partner":["Airtel","Airtel","Airtel","Reliance Jio","Vodafone"],"gender":["F","F","F","M","M"],"age":[26,74,54,29,45],"state":["Himachal Pradesh","Uttarakhand","Jharkhand","Bihar","Nagaland"],"city":["Delhi","Hyderabad","Chennai","Hyderabad","Bangalore"],"num_dependents":[4,0,2,3,4],"estimated_salary":[85979,69445,75949,34272,34157],"calls_made":[75,35,70,95,66],"sms_sent":[21,38,47,32,23],"data_used":[4532,723,4688,10241,5246],"days_since_registration":[1985,1314,1319,1123,1990]}},"total_rows":5,"truncation_type":null},"text/plain":"  telecom_partner gender  age  ... sms_sent data_used  days_since_registration\n0          Airtel      F   26  ...       21      4532                     1985\n1          Airtel      F   74  ...       38       723                     1314\n2          Airtel      F   54  ...       47      4688                     1319\n3    Reliance Jio      M   29  ...       32     10241                     1123\n4        Vodafone      M   45  ...       23      5246                     1990\n\n[5 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>telecom_partner</th>\n      <th>gender</th>\n      <th>age</th>\n      <th>state</th>\n      <th>city</th>\n      <th>num_dependents</th>\n      <th>estimated_salary</th>\n      <th>calls_made</th>\n      <th>sms_sent</th>\n      <th>data_used</th>\n      <th>days_since_registration</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Airtel</td>\n      <td>F</td>\n      <td>26</td>\n      <td>Himachal Pradesh</td>\n      <td>Delhi</td>\n      <td>4</td>\n      <td>85979</td>\n      <td>75</td>\n      <td>21</td>\n      <td>4532</td>\n      <td>1985</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Airtel</td>\n      <td>F</td>\n      <td>74</td>\n      <td>Uttarakhand</td>\n      <td>Hyderabad</td>\n      <td>0</td>\n      <td>69445</td>\n      <td>35</td>\n      <td>38</td>\n      <td>723</td>\n      <td>1314</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Airtel</td>\n      <td>F</td>\n      <td>54</td>\n      <td>Jharkhand</td>\n      <td>Chennai</td>\n      <td>2</td>\n      <td>75949</td>\n      <td>70</td>\n      <td>47</td>\n      <td>4688</td>\n      <td>1319</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Reliance Jio</td>\n      <td>M</td>\n      <td>29</td>\n      <td>Bihar</td>\n      <td>Hyderabad</td>\n      <td>3</td>\n      <td>34272</td>\n      <td>95</td>\n      <td>32</td>\n      <td>10241</td>\n      <td>1123</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Vodafone</td>\n      <td>M</td>\n      <td>45</td>\n      <td>Nagaland</td>\n      <td>Bangalore</td>\n      <td>4</td>\n      <td>34157</td>\n      <td>66</td>\n      <td>23</td>\n      <td>5246</td>\n      <td>1990</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":105}],"execution_count":105},{"source":"# scale relevant numeric columns\nnumeric_columns = ['age', 'num_dependents', 'estimated_salary', 'calls_made', 'sms_sent', 'data_used', 'days_since_registration']\nscaler = StandardScaler()\nscaled_num = pd.DataFrame(\n    scaler.fit_transform(churn_df[numeric_columns]),\n    columns=numeric_columns,\n    index=churn_df.index)","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1755869442262,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# scale relevant numeric columns\nnumeric_columns = ['age', 'num_dependents', 'estimated_salary', 'calls_made', 'sms_sent', 'data_used', 'days_since_registration']\nscaler = StandardScaler()\nscaled_num = pd.DataFrame(\n    scaler.fit_transform(churn_df[numeric_columns]),\n    columns=numeric_columns,\n    index=churn_df.index)","outputsMetadata":{"0":{"height":550,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"ec78ef7b-1bac-4329-9e40-729404e9400d","nodeType":"const"}}}}},"cell_type":"code","id":"a0bc8279-9015-44f6-895c-7c607cc62e24","outputs":[],"execution_count":106},{"source":"features_scaled = pd.concat([scaled_num, dummies], axis=1)","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1755869442310,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"features_scaled = pd.concat([scaled_num, dummies], axis=1)","outputsMetadata":{"0":{"height":500,"type":"dataFrame","tableState":{}},"1":{"height":500,"type":"dataFrame","tableState":{}}}},"cell_type":"code","id":"c265f971-d9a2-4fc2-81ab-f22f65400e52","outputs":[],"execution_count":107},{"source":"print(\"Features shape:\", features_scaled.shape)\nprint(\"Target shape:\", target.shape)\ndisplay(features_scaled.head())","metadata":{"executionCancelledAt":null,"executionTime":61,"lastExecutedAt":1755869442371,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(\"Features shape:\", features_scaled.shape)\nprint(\"Target shape:\", target.shape)\ndisplay(features_scaled.head())","outputsMetadata":{"0":{"height":59,"type":"stream"},"1":{"height":500,"type":"dataFrame","tableState":{}}}},"cell_type":"code","id":"7783779e-7fff-4b68-8b87-7f15ddea47f2","outputs":[{"output_type":"stream","name":"stdout","text":"Features shape: (6500, 43)\nTarget shape: (6500,)\n"},{"output_type":"display_data","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"age","type":"number"},{"name":"num_dependents","type":"number"},{"name":"estimated_salary","type":"number"},{"name":"calls_made","type":"number"},{"name":"sms_sent","type":"number"},{"name":"data_used","type":"number"},{"name":"days_since_registration","type":"number"},{"name":"telecom_partner_BSNL","type":"integer"},{"name":"telecom_partner_Reliance Jio","type":"integer"},{"name":"telecom_partner_Vodafone","type":"integer"},{"name":"gender_M","type":"integer"},{"name":"state_Arunachal Pradesh","type":"integer"},{"name":"state_Assam","type":"integer"},{"name":"state_Bihar","type":"integer"},{"name":"state_Chhattisgarh","type":"integer"},{"name":"state_Goa","type":"integer"},{"name":"state_Gujarat","type":"integer"},{"name":"state_Haryana","type":"integer"},{"name":"state_Himachal Pradesh","type":"integer"},{"name":"state_Jharkhand","type":"integer"},{"name":"state_Karnataka","type":"integer"},{"name":"state_Kerala","type":"integer"},{"name":"state_Madhya Pradesh","type":"integer"},{"name":"state_Maharashtra","type":"integer"},{"name":"state_Manipur","type":"integer"},{"name":"state_Meghalaya","type":"integer"},{"name":"state_Mizoram","type":"integer"},{"name":"state_Nagaland","type":"integer"},{"name":"state_Odisha","type":"integer"},{"name":"state_Punjab","type":"integer"},{"name":"state_Rajasthan","type":"integer"},{"name":"state_Sikkim","type":"integer"},{"name":"state_Tamil Nadu","type":"integer"},{"name":"state_Telangana","type":"integer"},{"name":"state_Tripura","type":"integer"},{"name":"state_Uttar Pradesh","type":"integer"},{"name":"state_Uttarakhand","type":"integer"},{"name":"state_West Bengal","type":"integer"},{"name":"city_Chennai","type":"integer"},{"name":"city_Delhi","type":"integer"},{"name":"city_Hyderabad","type":"integer"},{"name":"city_Kolkata","type":"integer"},{"name":"city_Mumbai","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2,3,4],"age":[-1.2229697881,1.6963038022,0.4799398062,-1.0405151887,-0.067423992],"num_dependents":[1.43653887,-1.4113460419,0.012596414,0.724567642,1.43653887],"estimated_salary":[0.0119811842,-0.4284234101,-0.2551809109,-1.3653020116,-1.3683651862],"calls_made":[0.8460758531,-0.4963443986,0.6782733216,1.5172859789,0.5440312964],"sms_sent":[-0.2223845072,0.9380559936,1.552406847,0.528488758,-0.0858620953],"data_used":[-0.1594880239,-1.4548963021,-0.10643376,1.7820939819,0.0833372611],"days_since_registration":[1.5060770589,-0.3996410898,-0.3854405075,-0.9421033348,1.5202776412],"telecom_partner_BSNL":[0,0,0,0,0],"telecom_partner_Reliance Jio":[0,0,0,1,0],"telecom_partner_Vodafone":[0,0,0,0,1],"gender_M":[0,0,0,1,1],"state_Arunachal Pradesh":[0,0,0,0,0],"state_Assam":[0,0,0,0,0],"state_Bihar":[0,0,0,1,0],"state_Chhattisgarh":[0,0,0,0,0],"state_Goa":[0,0,0,0,0],"state_Gujarat":[0,0,0,0,0],"state_Haryana":[0,0,0,0,0],"state_Himachal Pradesh":[1,0,0,0,0],"state_Jharkhand":[0,0,1,0,0],"state_Karnataka":[0,0,0,0,0],"state_Kerala":[0,0,0,0,0],"state_Madhya Pradesh":[0,0,0,0,0],"state_Maharashtra":[0,0,0,0,0],"state_Manipur":[0,0,0,0,0],"state_Meghalaya":[0,0,0,0,0],"state_Mizoram":[0,0,0,0,0],"state_Nagaland":[0,0,0,0,1],"state_Odisha":[0,0,0,0,0],"state_Punjab":[0,0,0,0,0],"state_Rajasthan":[0,0,0,0,0],"state_Sikkim":[0,0,0,0,0],"state_Tamil Nadu":[0,0,0,0,0],"state_Telangana":[0,0,0,0,0],"state_Tripura":[0,0,0,0,0],"state_Uttar Pradesh":[0,0,0,0,0],"state_Uttarakhand":[0,1,0,0,0],"state_West Bengal":[0,0,0,0,0],"city_Chennai":[0,0,1,0,0],"city_Delhi":[1,0,0,0,0],"city_Hyderabad":[0,1,0,1,0],"city_Kolkata":[0,0,0,0,0],"city_Mumbai":[0,0,0,0,0]}},"total_rows":5,"truncation_type":null},"text/plain":"        age  num_dependents  ...  city_Kolkata  city_Mumbai\n0 -1.222970        1.436539  ...             0            0\n1  1.696304       -1.411346  ...             0            0\n2  0.479940        0.012596  ...             0            0\n3 -1.040515        0.724568  ...             0            0\n4 -0.067424        1.436539  ...             0            0\n\n[5 rows x 43 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>num_dependents</th>\n      <th>estimated_salary</th>\n      <th>calls_made</th>\n      <th>sms_sent</th>\n      <th>data_used</th>\n      <th>days_since_registration</th>\n      <th>telecom_partner_BSNL</th>\n      <th>telecom_partner_Reliance Jio</th>\n      <th>telecom_partner_Vodafone</th>\n      <th>gender_M</th>\n      <th>state_Arunachal Pradesh</th>\n      <th>state_Assam</th>\n      <th>state_Bihar</th>\n      <th>state_Chhattisgarh</th>\n      <th>state_Goa</th>\n      <th>state_Gujarat</th>\n      <th>state_Haryana</th>\n      <th>state_Himachal Pradesh</th>\n      <th>state_Jharkhand</th>\n      <th>state_Karnataka</th>\n      <th>state_Kerala</th>\n      <th>state_Madhya Pradesh</th>\n      <th>state_Maharashtra</th>\n      <th>state_Manipur</th>\n      <th>state_Meghalaya</th>\n      <th>state_Mizoram</th>\n      <th>state_Nagaland</th>\n      <th>state_Odisha</th>\n      <th>state_Punjab</th>\n      <th>state_Rajasthan</th>\n      <th>state_Sikkim</th>\n      <th>state_Tamil Nadu</th>\n      <th>state_Telangana</th>\n      <th>state_Tripura</th>\n      <th>state_Uttar Pradesh</th>\n      <th>state_Uttarakhand</th>\n      <th>state_West Bengal</th>\n      <th>city_Chennai</th>\n      <th>city_Delhi</th>\n      <th>city_Hyderabad</th>\n      <th>city_Kolkata</th>\n      <th>city_Mumbai</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.222970</td>\n      <td>1.436539</td>\n      <td>0.011981</td>\n      <td>0.846076</td>\n      <td>-0.222385</td>\n      <td>-0.159488</td>\n      <td>1.506077</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.696304</td>\n      <td>-1.411346</td>\n      <td>-0.428423</td>\n      <td>-0.496344</td>\n      <td>0.938056</td>\n      <td>-1.454896</td>\n      <td>-0.399641</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.479940</td>\n      <td>0.012596</td>\n      <td>-0.255181</td>\n      <td>0.678273</td>\n      <td>1.552407</td>\n      <td>-0.106434</td>\n      <td>-0.385441</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.040515</td>\n      <td>0.724568</td>\n      <td>-1.365302</td>\n      <td>1.517286</td>\n      <td>0.528489</td>\n      <td>1.782094</td>\n      <td>-0.942103</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.067424</td>\n      <td>1.436539</td>\n      <td>-1.368365</td>\n      <td>0.544031</td>\n      <td>-0.085862</td>\n      <td>0.083337</td>\n      <td>1.520278</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}}}],"execution_count":108},{"source":"### 3. Train-test-split","metadata":{},"cell_type":"markdown","id":"cd8cec54-70eb-4c4d-a0f7-8de1a1b7b85e"},{"source":"# Split into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(\n    features_scaled, \n    target, \n    test_size=0.2,      # 20% for testing\n    random_state=42,    # ensures reproducibility\n)\n\nprint(\"Train shape:\", X_train.shape, y_train.shape)\nprint(\"Test shape:\", X_test.shape, y_test.shape)","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1755869442418,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Split into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(\n    features_scaled, \n    target, \n    test_size=0.2,      # 20% for testing\n    random_state=42,    # ensures reproducibility\n)\n\nprint(\"Train shape:\", X_train.shape, y_train.shape)\nprint(\"Test shape:\", X_test.shape, y_test.shape)","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"db67228e-c8d2-4e06-b48b-5bc8c1314f96","outputs":[{"output_type":"stream","name":"stdout","text":"Train shape: (5200, 43) (5200,)\nTest shape: (1300, 43) (1300,)\n"}],"execution_count":109},{"source":"# investigate the class imbalance in train and test set\nprint(f\"Proportion of customers churned in train_set: {str(y_train.mean())}\")\nprint(f\"Proportion of customers churned in test_set: {str(y_train.mean())}\")","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1755869442471,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# investigate the class imbalance in train and test set\nprint(f\"Proportion of customers churned in train_set: {str(y_train.mean())}\")\nprint(f\"Proportion of customers churned in test_set: {str(y_train.mean())}\")"},"cell_type":"code","id":"b3ab231c-9ef1-445f-a68d-93e06d3c5db6","outputs":[{"output_type":"stream","name":"stdout","text":"Proportion of customers churned in train_set: 0.19807692307692307\nProportion of customers churned in test_set: 0.19807692307692307\n"}],"execution_count":110},{"source":"### 4. Training the models and getting predictions","metadata":{},"cell_type":"markdown","id":"82d9f4c5-2009-4b41-af4f-0a553804211d"},{"source":"# instantiate and fit each model\n\n# logistic regression\nlog_reg = RidgeClassifier(random_state = 42)\nlog_reg.fit(X_train, y_train)\nlogreg_pred = log_reg.predict(X_test)\nprint(\"Show value counts of predictions\")\nprint(np.unique(logreg_pred, return_counts=True))\n\n# random forest\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\nprint(\"Show value counts of predictions\")\nprint(np.unique(rf_pred, return_counts=True))","metadata":{"executionCancelledAt":null,"executionTime":1016,"lastExecutedAt":1755869443487,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# instantiate and fit each model\n\n# logistic regression\nlog_reg = RidgeClassifier(random_state = 42)\nlog_reg.fit(X_train, y_train)\nlogreg_pred = log_reg.predict(X_test)\nprint(\"Show value counts of predictions\")\nprint(np.unique(logreg_pred, return_counts=True))\n\n# random forest\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\nprint(\"Show value counts of predictions\")\nprint(np.unique(rf_pred, return_counts=True))","outputsMetadata":{"0":{"height":59,"type":"stream"},"2":{"height":38,"type":"stream"}}},"cell_type":"code","id":"fbf45598-b3b0-4141-b6a3-4fcbee85bd38","outputs":[{"output_type":"stream","name":"stdout","text":"Show value counts of predictions\n(array([0]), array([1300]))\nShow value counts of predictions\n(array([0, 1]), array([1299,    1]))\n"}],"execution_count":111},{"source":"### 5. Assessing the models","metadata":{},"cell_type":"markdown","id":"2e98a306-27cf-461f-996c-aa1d9120d0e0"},{"source":"# confusion matrices\n## logistic regression\nprint(\"Performance of logistic regression\")\ncm = confusion_matrix(y_test, logreg_pred)\n# print confustion matrix with labels\ncm_df = pd.DataFrame(\n    cm,\n    index=[f\"Actual {label}\" for label in log_reg.classes_],\n    columns=[f\"Predicted {label}\" for label in log_reg.classes_]\n)\nprint(cm_df)\nprint(classification_report(y_test, logreg_pred))\n\n## random forest\nprint(\"Performance of random forest\")\ncm = confusion_matrix(y_test, rf_pred)\ncm_df = pd.DataFrame(\n    cm,\n    index=[f\"Actual {label}\" for label in log_reg.classes_],\n    columns=[f\"Predicted {label}\" for label in log_reg.classes_]\n)\nprint(cm_df)\nprint(classification_report(y_test, rf_pred))","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1755869443535,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# confusion matrices\n## logistic regression\nprint(\"Performance of logistic regression\")\ncm = confusion_matrix(y_test, logreg_pred)\n# print confustion matrix with labels\ncm_df = pd.DataFrame(\n    cm,\n    index=[f\"Actual {label}\" for label in log_reg.classes_],\n    columns=[f\"Predicted {label}\" for label in log_reg.classes_]\n)\nprint(cm_df)\nprint(classification_report(y_test, logreg_pred))\n\n## random forest\nprint(\"Performance of random forest\")\ncm = confusion_matrix(y_test, rf_pred)\ncm_df = pd.DataFrame(\n    cm,\n    index=[f\"Actual {label}\" for label in log_reg.classes_],\n    columns=[f\"Predicted {label}\" for label in log_reg.classes_]\n)\nprint(cm_df)\nprint(classification_report(y_test, rf_pred))","outputsMetadata":{"0":{"height":521,"type":"stream"}}},"cell_type":"code","id":"24b5b8b9-c32f-4f31-a191-5d696f9cdbf7","outputs":[{"output_type":"stream","name":"stdout","text":"Performance of logistic regression\n          Predicted 0  Predicted 1\nActual 0         1027            0\nActual 1          273            0\n              precision    recall  f1-score   support\n\n           0       0.79      1.00      0.88      1027\n           1       0.00      0.00      0.00       273\n\n    accuracy                           0.79      1300\n   macro avg       0.40      0.50      0.44      1300\nweighted avg       0.62      0.79      0.70      1300\n\nPerformance of random forest\n          Predicted 0  Predicted 1\nActual 0         1026            1\nActual 1          273            0\n              precision    recall  f1-score   support\n\n           0       0.79      1.00      0.88      1027\n           1       0.00      0.00      0.00       273\n\n    accuracy                           0.79      1300\n   macro avg       0.39      0.50      0.44      1300\nweighted avg       0.62      0.79      0.70      1300\n\n"}],"execution_count":112},{"source":"## Preliminary conclusion:\nBoth models can't handle class imbalance and predict a 0 for each of the observations. While the accuracy of 80% seems good, it is misleading because simply predicting every observation to 0 leads to this accuracy. \n\n### Solution:\nFirst, I have to handle the class imbalance in the train set. This can be most easily solved by undersampling the majority class. The true occurance of churn is around 20% and the train dataset contains more than 5000 observations, this would leave me with enough data to train the model.","metadata":{},"cell_type":"markdown","id":"ec25855a-a7e8-499b-bb64-072d8c5f8e2f"},{"source":"### 6a. Undersampling majority class","metadata":{},"cell_type":"markdown","id":"67b4f59b-5222-47d5-a464-c8c00bf5a653"},{"source":"np.random.seed(42)\n\n# undersample X_train and y_train\n# Find the class counts\nclass_counts = y_train.value_counts()\nprint(\"Class distribution before undersampling:\\n\", class_counts)\n\n# Identify majority and minority classes\nmajority_class = class_counts.idxmax()\nminority_class = class_counts.idxmin()\nprint(f\"Majority class: {majority_class}, Minority class: {minority_class}\")\n\n# Get indices of the majoriy and minority class\nmajority_indices = y_train[y_train == majority_class].index\nminority_indices = y_train[y_train == minority_class].index\n\n# Randomly sample majority class to match minority size\nundersampled_majority_indices = np.random.choice(\n    majority_indices,\n    size=len(minority_indices),\n    replace=False\n)\n\nprint(f\"Number of samples from majority class: {len(undersampled_majority_indices)}\")\n\n# Combine undersampled majority + all minority\nundersampled_indices = np.concatenate([undersampled_majority_indices, minority_indices])\n\n# Subset the training data\nX_train_under = X_train.loc[undersampled_indices]\ny_train_under = y_train.loc[undersampled_indices]\n\nprint(f'Shape of train data: {X_train_under.shape}')\nprint(\"Class distribution after undersampling:\\n\", y_train_under.value_counts())","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1755869443587,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"np.random.seed(42)\n\n# undersample X_train and y_train\n# Find the class counts\nclass_counts = y_train.value_counts()\nprint(\"Class distribution before undersampling:\\n\", class_counts)\n\n# Identify majority and minority classes\nmajority_class = class_counts.idxmax()\nminority_class = class_counts.idxmin()\nprint(f\"Majority class: {majority_class}, Minority class: {minority_class}\")\n\n# Get indices of the majoriy and minority class\nmajority_indices = y_train[y_train == majority_class].index\nminority_indices = y_train[y_train == minority_class].index\n\n# Randomly sample majority class to match minority size\nundersampled_majority_indices = np.random.choice(\n    majority_indices,\n    size=len(minority_indices),\n    replace=False\n)\n\nprint(f\"Number of samples from majority class: {len(undersampled_majority_indices)}\")\n\n# Combine undersampled majority + all minority\nundersampled_indices = np.concatenate([undersampled_majority_indices, minority_indices])\n\n# Subset the training data\nX_train_under = X_train.loc[undersampled_indices]\ny_train_under = y_train.loc[undersampled_indices]\n\nprint(f'Shape of train data: {X_train_under.shape}')\nprint(\"Class distribution after undersampling:\\n\", y_train_under.value_counts())","outputsMetadata":{"0":{"height":500,"type":"dataFrame","tableState":{}},"1":{"height":500,"type":"dataFrame","tableState":{}}}},"cell_type":"code","id":"4458f830-6939-42dd-87e0-11b7cf3af942","outputs":[{"output_type":"stream","name":"stdout","text":"Class distribution before undersampling:\n 0    4170\n1    1030\nName: churn, dtype: int64\nMajority class: 0, Minority class: 1\nNumber of samples from majority class: 1030\nShape of train data: (2060, 43)\nClass distribution after undersampling:\n 0    1030\n1    1030\nName: churn, dtype: int64\n"}],"execution_count":113},{"source":"### 6b. Fit the models again","metadata":{},"cell_type":"markdown","id":"d71f2b6e-01cf-4c3f-b166-4ce33d649d3d"},{"source":"# instantiate and fit each model\n\n# logistic regression\nlog_reg = RidgeClassifier(random_state = 42)\nlog_reg.fit(X_train_under, y_train_under)\nlogreg_pred = log_reg.predict(X_test)\nprint(\"Show value counts of predictions\")\nprint(np.unique(logreg_pred, return_counts=True))\n\n# random forest\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train_under, y_train_under)\nrf_pred = rf_clf.predict(X_test)\nprint(\"Show value counts of predictions\")\nprint(np.unique(rf_pred, return_counts=True))","metadata":{"executionCancelledAt":null,"executionTime":543,"lastExecutedAt":1755869444130,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# instantiate and fit each model\n\n# logistic regression\nlog_reg = RidgeClassifier(random_state = 42)\nlog_reg.fit(X_train_under, y_train_under)\nlogreg_pred = log_reg.predict(X_test)\nprint(\"Show value counts of predictions\")\nprint(np.unique(logreg_pred, return_counts=True))\n\n# random forest\nrf_clf = RandomForestClassifier(random_state=42)\nrf_clf.fit(X_train_under, y_train_under)\nrf_pred = rf_clf.predict(X_test)\nprint(\"Show value counts of predictions\")\nprint(np.unique(rf_pred, return_counts=True))"},"cell_type":"code","id":"036f1d0b-bc91-4b2d-b246-ee1f9e6fff2d","outputs":[{"output_type":"stream","name":"stdout","text":"Show value counts of predictions\n(array([0, 1]), array([646, 654]))\nShow value counts of predictions\n(array([0, 1]), array([648, 652]))\n"}],"execution_count":114},{"source":"# confusion matrices\n## logistic regression\nprint(\"Performance of logistic regression\")\nprint(f\"Precision: {precision_score(y_test, logreg_pred)}\")\nprint(f\"Recall: {recall_score(y_test, logreg_pred)}\")\ncm = confusion_matrix(y_test, logreg_pred)\n# print confustion matrix with labels\ncm_df = pd.DataFrame(\n    cm,\n    index=[f\"Actual {label}\" for label in log_reg.classes_],\n    columns=[f\"Predicted {label}\" for label in log_reg.classes_]\n)\nprint(cm_df)\nprint(classification_report(y_test, logreg_pred))\n\n## random forest\nprint(\"Performance of random forest\")\nprint(f\"Precision: {precision_score(y_test, rf_pred)}\")\nprint(f\"Recall: {recall_score(y_test, rf_pred)}\")\ncm = confusion_matrix(y_test, rf_pred)\ncm_df = pd.DataFrame(\n    cm,\n    index=[f\"Actual {label}\" for label in log_reg.classes_],\n    columns=[f\"Predicted {label}\" for label in log_reg.classes_]\n)\nprint(cm_df)\nprint(classification_report(y_test, rf_pred))","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1755869444180,"lastExecutedByKernel":"5aecf7d6-4f9c-4c76-b1d4-acd0d6ec225b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# confusion matrices\n## logistic regression\nprint(\"Performance of logistic regression\")\nprint(f\"Precision: {precision_score(y_test, logreg_pred)}\")\nprint(f\"Recall: {recall_score(y_test, logreg_pred)}\")\ncm = confusion_matrix(y_test, logreg_pred)\n# print confustion matrix with labels\ncm_df = pd.DataFrame(\n    cm,\n    index=[f\"Actual {label}\" for label in log_reg.classes_],\n    columns=[f\"Predicted {label}\" for label in log_reg.classes_]\n)\nprint(cm_df)\nprint(classification_report(y_test, logreg_pred))\n\n## random forest\nprint(\"Performance of random forest\")\nprint(f\"Precision: {precision_score(y_test, rf_pred)}\")\nprint(f\"Recall: {recall_score(y_test, rf_pred)}\")\ncm = confusion_matrix(y_test, rf_pred)\ncm_df = pd.DataFrame(\n    cm,\n    index=[f\"Actual {label}\" for label in log_reg.classes_],\n    columns=[f\"Predicted {label}\" for label in log_reg.classes_]\n)\nprint(cm_df)\nprint(classification_report(y_test, rf_pred))"},"cell_type":"code","id":"08894f3d-32c0-4734-9b80-8a176cf8ec63","outputs":[{"output_type":"stream","name":"stdout","text":"Performance of logistic regression\nPrecision: 0.2018348623853211\nRecall: 0.4835164835164835\n          Predicted 0  Predicted 1\nActual 0          505          522\nActual 1          141          132\n              precision    recall  f1-score   support\n\n           0       0.78      0.49      0.60      1027\n           1       0.20      0.48      0.28       273\n\n    accuracy                           0.49      1300\n   macro avg       0.49      0.49      0.44      1300\nweighted avg       0.66      0.49      0.54      1300\n\nPerformance of random forest\nPrecision: 0.200920245398773\nRecall: 0.47985347985347987\n          Predicted 0  Predicted 1\nActual 0          506          521\nActual 1          142          131\n              precision    recall  f1-score   support\n\n           0       0.78      0.49      0.60      1027\n           1       0.20      0.48      0.28       273\n\n    accuracy                           0.49      1300\n   macro avg       0.49      0.49      0.44      1300\nweighted avg       0.66      0.49      0.54      1300\n\n"}],"execution_count":115},{"source":"## Conclusion:\nThe performance of both models is very similar. \nI am mostly interested in predicting the actual churn correctly, which means that I prefer True Positives at the cost of False Positives. Thus I need a model with a higher recall.   \nThe logistic regression has the best performance.","metadata":{},"cell_type":"markdown","id":"53d8e8de-6257-45c6-b9b3-1db162fc4743"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}